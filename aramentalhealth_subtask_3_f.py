# -*- coding: utf-8 -*-
"""AraMentalHealth_SubTask_3_f.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xP18Ay4PCmklo6IfdoUGlf2C3oTGnXR5

#  MentalQA Dataset Description

The full dataset consists of **500 annotated samples**. For the purpose of this shared task, we are using only the **training and development set**:

- **Train+Dev Set**: 350 samples  
- **Test Set**: 150 samples (withheld for evaluation purposes)

---

###  Dataset File: `MentalQA_500_data.csv` (tab-separated)

This dataset was recently accepted in the *IEEE ACCESS* journal.  
It contains **four columns** as follows:

1. **Question**  
2. **Answer**  
3. **Question Types** (`final_QT`)  
4. **Answer Strategies** (`final_AS`)

---

###  Question Type Labels (`final_QT`)

| Label | Name                       |Description |
|----------|----------------------------|----------------
| A        | **Diagnosis**              | Questions about clinical findings, tests, disease criteria, and manifestations. |
| B        | **Treatment**              | Questions about therapies, drug use, side effects, and contraindications. |
| C        | **Anatomy and Physiology** | Basic medical knowledge (e.g., tissues, organs, metabolism). |
| D        | **Epidemiology**           | Disease progression, prognosis, etiology, and risk factors. |
| E        | **Healthy Lifestyle**      | Diet, exercise, mood, and lifestyle factors affecting mental health. |
| F        | **Provider Choices**       | Recommendations for doctors, hospitals, or medical departments. |
| Z        | **Other**                  | General or uncategorized questions. |

---

###  Answer Strategy Labels (`final_AS`)

| Label   | Name              | Description |
|---------|-----------|-------------|
| 1       | **Information**    | Provides factual data, explanations, or resources. |
| 2       |  **Direct Guidance**| Offers suggestions, instructions, or behavioral advice. |
| 3       | **Emotional Support** | Gives reassurance, empathy, or emotional encouragement. |





GitHub: https://github.com/hasanhuz/MentalQA/tree/main

Paper: https://arxiv.org/abs/2405.12619
"""

!pip install openai==0.28
!pip install bert-score
!pip install -U langchain langchain-community
!pip install -U langchain-groq

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

pd.set_option("display.max_colwidth", None, "display.max_rows", None)

data = pd.read_csv('/content/Train_Dev.tsv', sep='\t')
data.sample(3)

"""### Start Cleaning Dataset
##  Data Preprocessing and Cleaning Overview

Before performing any analysis or model evaluation, it is crucial to clean and normalize the dataset to ensure consistency and quality.

This section outlines the preprocessing steps applied to the MentalQA Arabic dataset.

---

###  Cleaning Steps Applied:

1. **Displayed full text columns** â€“ Enabled full visibility of long question/answer text using `pd.set_option("display.max_colwidth", None)`.
2. **Email Removal** â€“ Removed any embedded email addresses using regular expressions.
3. **Separation Rules** â€“ Inserted spaces between:
   - Arabic letters and numbers (e.g., "Ø¯ÙˆØ§Ø¡20" â†’ "Ø¯ÙˆØ§Ø¡ 20")
   - Arabic and English terms (e.g., "Ø¹Ù„Ø§Ø¬mg" â†’ "Ø¹Ù„Ø§Ø¬ mg")
   - Numbers and English words (e.g., "mg75" â†’ "mg 75")
4. **Punctuation Cleanup** â€“ Removed all punctuation marks except the Arabic question mark "ØŸ".
5. **Arabic Letter Normalization** â€“ Standardized variations of alef (Ø§), ta marbuta (Ù‡), and others.
6. **Numeric Normalization** â€“ Converted Arabic-Indic numerals (Ù Ù¡Ù¢...) to Western digits (012...).
7. **Repetition Reduction** â€“ Compressed elongated words (e.g., "Ø±Ø§Ø§Ø§Ø§Ø§Ø¦Ø¹" â†’ "Ø±Ø§Ø¦Ø¹").
8. **Question Mark Spacing** â€“ Ensured space before and after "ØŸ" to avoid it sticking to adjacent words.
9. **Custom Corrections** â€“ Applied multiple custom dictionaries to fix common spelling errors, typos, and fused terms (e.g., "Ù…Ø§Ø§" â†’ "Ù…Ø§", "Ø¨Ù„Ù†ØªØ­Ø§Ø±" â†’ "Ø¨Ø§Ù„Ø§Ù†ØªØ­Ø§Ø±").

---

###  Word-Level Error Detection via Frequency Analysis

To further enhance text quality, we analyzed both the cleaned questions and answers at the word level:

- Combined all tokens from `question_clean_1` and `answer_clean_1`.
- Used `collections.Counter` to calculate word frequencies.
- Created a sorted DataFrame to highlight rare and suspicious words.
- Manually reviewed low-frequency words to detect:
  - Typos (e.g., "Ù‡Ø§Ø°Ø§", "Ù…Ø¹Ø§ÙŠØ§", "Ù…ÙØ§Ø±Ù‚Ù‡Ù…ØªØµÙ„")
  - Word merges and informal language
- Built custom correction dictionaries based on the findings to normalize the vocabulary.

This iterative refinement helped reduce vocabulary noise and improve input quality for downstream modeling.

"""

data.columns

data.shape

df = data[['question', 'answer']]
df

import re

def clean_arabic_text(text):
    if pd.isnull(text):
        return ""
    # 0. Normalize numbers
    text = text.translate(str.maketrans("Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©", "0123456789"))

    # 1. Remove Email (ÙˆØ§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡ Ø¨Ù…Ø³Ø§ÙØ©)
    text = re.sub(r'\S*@\S+', ' ', text)

    # 2. Make Distance between Arabic and Numbers/English
    text = re.sub(r'([Ø¡-ÙŠ])(\d+)', r'\1 \2', text)
    text = re.sub(r'(\d+)([Ø¡-ÙŠ])', r'\1 \2', text)
    text = re.sub(r'([Ø¡-ÙŠ])([a-zA-Z]+)', r'\1 \2', text)
    text = re.sub(r'([a-zA-Z]+)([Ø¡-ÙŠ])', r'\1 \2', text)
    text = re.sub(r'([a-zA-Z]+)(\d+)', r'\1 \2', text)
    text = re.sub(r'(\d+)([a-zA-Z]+)', r'\1 \2', text)

    # 3. Remove repeated characters (Ù…Ø«Ù„Ø§Ù‹: ÙƒÙŠÙŠÙŠÙŠÙ â†’ ÙƒÙŠÙ)
    text = re.sub(r'(.)\1{2,}', r'\1', text)

    # 4. Normalize Arabic letters
    text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)
    text = re.sub(r'Ø©', 'Ù‡', text)
    text = re.sub(r'Ú¯', 'Ùƒ', text)

    # 5. Remove punctuation (except ØŸ) Ù…Ø¹ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡Ø§ Ø¨Ù…Ø³Ø§ÙØ©
    text = re.sub(r'[^\w\sØŸ]', ' ', text)

    # 6. Ensure space before and after question mark
    text = re.sub(r'\s*ØŸ\s*', ' ØŸ ', text)

    # 7. Remove extra whitespaces
    text = re.sub(r'\s+', ' ', text).strip()

    return text

df["question_clean_1"] = df["question"].apply(clean_arabic_text)
df["answer_clean_1"] = df["answer"].apply(clean_arabic_text)

all_text = ' '.join(df['question_clean_1'].astype(str))
words = all_text.split()
word_freq = Counter(words)
freq_df = pd.DataFrame(word_freq.items(), columns=['word', 'count']).sort_values(by='count', ascending=True)
freq_df

def apply_custom_corrections(text):
    corrections = {
        "Ù‚ÙŠ": "ÙÙŠ",
        "Ø§Ù†Ø§Ø§Ø°ÙƒØ±": "Ø£Ù†Ø§ Ø£Ø°ÙƒØ±",
        "Ù…Ø§Ø§": "Ù…Ø§",
        "Ø¹Ù„ÙŠÙƒÙ…Ø§Ø¹Ø§Ù†ÙŠ": "Ø¹Ù„ÙŠÙƒÙ… Ø§Ø¹Ø§Ù†ÙŠ",
        "Ù‡Ø¯Ù‡": "Ù‡Ø°Ù‡",
        "Ù„Ù„Ø§ÙƒØªØ¦Ø§": "Ù„Ù„Ø§ÙƒØªØ¦Ø§Ø¨",
        "Ø¨Ù„Ø³ÙˆØ¡": "Ø¨ Ø§Ù„Ø³ÙˆØ¡",
        "Ø¡Ø§Ø°ÙŠ": "Ø§Ù„Ø°ÙŠ",
        "Ù…Ø§ÙƒØ¯Ø±": "Ù…Ø§ Ø§Ù‚Ø¯Ø±",
        "Ù…Ø¹Ø§ÙŠØ§": "Ù…Ø¹ÙŠ",
        "Ø­ØªØ§": "Ø­ØªÙ‰",
        "Ø§Ù†ÙˆØ§": "Ø£Ù†Ù‡",
        "Ø§Ø®Ø±ÙŠ": "Ø£Ø®Ø±Ù‰",
        "Ø¨Ù„Ù†ØªØ­Ø§Ø±": "Ø¨ Ø§Ù„Ø§Ù†ØªØ­Ø§Ø±",
        "Ø¨Ù„Ø§Ø­Ø¨Ø§Ø·": "Ø¨ Ø§Ù„Ø¥Ø­Ø¨Ø§Ø·",
        "Ù…Ù†Ù‡Ø§Ø§": "Ù…Ù†Ù‡Ø§",
        "Ø§Ø¨ØºØ§": "Ø£Ø±ÙŠØ¯",
        "Ø·Ù…Ø§Ù†ØªÙ†ÙŠ": "Ø·Ù…Ù†ØªÙ†ÙŠ",
        "ÙˆÙˆØ§Ø­Ø¨Ù‡Ø§": "ÙˆØ§Ø­Ø¨Ù‡Ø§",
        "Ù…ÙØ¹ÙˆÙ„Ø®": "Ù…ÙØ¹ÙˆÙ„Ù‡",

    }

    for wrong, right in corrections.items():
        text = text.replace(wrong, right)
    return text

df["question_clean_2"] = df["question_clean_1"].apply(apply_custom_corrections)
#df["answer_clean_2"] = df["answer_clean_1"].apply(clean_arabic_text)

all_text_ = ' '.join(df['answer_clean_1'].astype(str))
words_ = all_text_.split()
word_freq_ = Counter(words_)
freq_df_ = pd.DataFrame(word_freq_.items(), columns=['word', 'count']).sort_values(by='count', ascending=True)
freq_df_

def apply_answer_corrections(text):
    corrections = {
        "Ø§Ù„Ù†ÙØ³ÙŠÙˆØ±Ø¨Ù…Ø§": "Ø§Ù„Ù†ÙØ³ÙŠ ÙˆØ±Ø¨Ù…Ø§",
        "Ø§Ù„Ø¹Ù‚Ø§Ù‚ÙŠØ±ÙˆÙŠØ³ØªØ­Ø³Ù†": "Ø§Ù„Ø¹Ù‚Ø§Ù‚ÙŠØ± ÙˆÙŠØ³ØªØ­Ø³Ù†",
        "ÙˆØ¶Ø±ÙˆØ±ÙŠÙˆØ§ÙŠØ°Ø§Ø¡": "Ùˆ Ø¶Ø±ÙˆØ±ÙŠ Ùˆ Ø¥ÙŠØ°Ø§Ø¡",
        "ÙˆØ§Ù„Ø¬Ø±Ø¹Ù‡Ù„ÙŠÙ‚Ø¯Ø±Ù‡Ø§": "Ùˆ Ø§Ù„Ø¬Ø±Ø¹Ø© Ù„ÙŠÙ‚Ø¯Ø±Ù‡Ø§",
        "ØªØ³ÙˆØ¡ÙˆØ±Ø¨Ù…Ø§": "ØªØ³ÙˆØ¡ ÙˆØ±Ø¨Ù…Ø§",
        "ØªØºÙŠÙŠØ±Ø§Ùˆ": "ØªØºÙŠÙŠØ± Ø£Ùˆ",
        "Ø§Ø¨Ù‚": "Ø£Ø¨Ù‚Ù‰",
        "Ø§Ù„Ø§Ø¯ÙˆÙŠÙ‡Ù†Ø­ØªØ§Ø¬": "Ø§Ù„Ø£Ø¯ÙˆÙŠØ© Ù†Ø­ØªØ§Ø¬",
        "Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠÙ‡Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯": "Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ© Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯",
        "Ø§Ù„Ø­Ù„ØªØ¯Ø±ÙŠØ¬ÙŠØ§": "Ø§Ù„Ø­Ù„ ØªØ¯Ø±ÙŠØ¬Ø§",
        "Ù…ÙØ§Ø±Ù‚Ù‡Ø§Ù„ØªØ¹Ù„Ù‚": "Ù…ÙØ§Ø±Ù‚Ø© Ø§Ù„ØªØ¹Ù„Ù‚",
        "Ø®Ø·ÙŠØ±Ù‡Ù„Ø§Ù†Ù‡Ø§": "Ø®Ø·ÙŠØ±Ø© Ù„Ø£Ù†Ù‡Ø§",
        "Ù†ÙØ³ÙŠØ§Ù„Ø§ÙÙƒØ§Ø±Ø§Ø§Ù„Ø§Ù†ØªØ­Ø§Ø±ÙŠÙ‡": "Ù†ÙØ³ÙŠ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø§Ù†ØªØ­Ø§Ø±ÙŠØ©",
        "Ø§Ù„Ø®Ø¶Ø§Ø±Ù‚Ù„Ù„ÙŠ": "Ø§Ù„Ø®Ø¶Ø§Ø± Ù‚Ù„Ù„ÙŠ",
        "ØºÙ„Ù‰": "Ø¹Ù„Ù‰",
        "Ø§Ù„Ø¬ØªÙ…Ø§Ø¹ÙŠ": "Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ",
        "Ù„ÙƒÙ„ÙŠÙ‡Ù…Ø§ÙˆÙ†ØªÙˆÙ‚Ù": "Ù„ÙƒÙ„ÙŠÙ‡Ù…Ø§ Ùˆ Ù†ØªÙˆÙ‚Ù",
        "ÙˆØ§Ù†ÙˆØ§Ø¹Ù‡Ø§ÙˆÙ„ÙˆÙ…": "Ùˆ Ø£Ù†ÙˆØ§Ø¹Ù‡Ø§ Ùˆ Ù„ÙˆÙ…",
        "Ø§Ù„Ù†Ø§Ø³Ù†Ø­ØªØ§Ø¬": "Ø§Ù„Ù†Ø§Ø³ Ù†Ø­ØªØ§Ø¬",
        "Ø§Ù„Ù…Ø±Ø§Ø¡Ù‡": "Ø§Ù„Ù…Ø±Ø§Ø©",
        "Ù‡Ø§Ø°Ø§": "Ù‡Ø°Ø§",
        "Ø§Ù„Ù…Ø³Ø³Ø¨Ù‡": "Ø§Ù„Ù…Ø³Ø¨Ø¨Ù‡",
        "Ø§Ù‰": "Ø§ÙŠ",
        "Ø¹Ø¶ÙˆÙ‰": "Ø¹Ø¶ÙˆÙŠ",
        "Ø§Ù„Ù†ÙØ³Ø§Ù†ÙŠØ§Ù„Ø§ÙÙƒØ§Ø±": "Ø§Ù„Ù†ÙØ³Ø§Ù†ÙŠ Ø§Ù„Ø£ÙÙƒØ§Ø±",
        "Ù‡Ø¯Ø§": "Ù‡Ø°Ø§",
        "Ø§Ø³Ø±Øº": "Ø§Ø³Ø±Ø¹",
        "ÙˆØ§Ù„Ø§Ø³ØªØ±Ø®Ø§Ø¡ÙˆÙ‡Ø°Ø§": "Ùˆ Ø§Ù„Ø§Ø³ØªØ±Ø®Ø§Ø¡ Ùˆ Ù‡Ø°Ø§",
        "Ø§Ù„Ø§Ø®Ø±ÙØªØµÙ„": "Ø§Ù„Ø§Ø®Ø± ÙØªØµÙ„",
        "Ø§Ù„Ø§Ø®Ø±ÙˆØ¹Ù†Ø¯Ù…Ø§": "Ø§Ù„Ø§Ø®Ø± Ùˆ Ø¹Ù†Ø¯Ù…Ø§",
        "Ø¯ÙˆØ§Ø¡ÙŠ": "Ø¯ÙˆØ§Ø¦ÙŠ",
        "Ù…Ø´Ù‡ÙˆØ±Ù‡Ø§Ø³Ù…Ù‡Ø§": "Ù…Ø´Ù‡ÙˆØ± Ø§Ø³Ù…Ù‡"
    }

    for wrong, right in corrections.items():
        text = text.replace(wrong, right)
    return text

df["answer_clean_2"] = df["answer_clean_1"].apply(apply_answer_corrections)

# Cleaning Columns
df[['question_clean_2', 'answer_clean_2']]

GROQ_API_KEY="gsk_...."
from langchain_groq import ChatGroq
from langchain.schema import HumanMessage


chat = ChatGroq(
    groq_api_key=GROQ_API_KEY,
    model_name="llama3-70b-8192",
    temperature=0.1,
    max_tokens=1024
)

def build_zero_shot_prompt(question):
    return f"Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø§Ø­ØªØ±Ø§ÙÙŠØ© ÙˆØ¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:\n\nØ§Ù„Ø³Ø¤Ø§Ù„: {question}\n\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"

def generate_llama_answer(question):
    prompt = build_zero_shot_prompt(question)
    response = chat.invoke([
        HumanMessage(content=prompt)
    ])
    return response.content.strip()

sample_df = df[['question_clean_2', 'answer_clean_2']].sample(n=10, random_state=42).copy()
sample_df['llama_zero_shot_answer'] = sample_df['question_clean_2'].apply(generate_llama_answer)
sample_df.to_csv("llama3_zero_shot_generated_answers.csv", index=False)

from bert_score import score

P, R, F1 = score(
    sample_df['llama_zero_shot_answer'].tolist(),
    sample_df['answer_clean_2'].tolist(),
    lang="ar",
    verbose=True
)

sample_df['bertscore_f1'] = F1.tolist()
sample_df.to_csv("llama3_zero_shot_generated_answers.csv", index=False)
print(f"âœ… Average BERTScore F1: {F1.mean().item():.2f}")

example_1 = {
    "question": "Ø¨Ù‚Ø§Ù„ÙŠ Ø§ÙƒØªØ± Ù…Ù† Ø§Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù…Ø´ Ø¨Ù†Ø§Ù… ÙˆÙ„Ø§ Ø¨Ø§ÙƒÙ„ Ùˆ Ù„Ùˆ Ù†Ù…Øª Ø³Ø§Ø¹Ù‡ ÙˆØ§Ø­Ø¯Ù‡ ÙÙŠ Ø§Ù„ÙŠÙˆÙ… Ø§ÙØ¶Ù„ ØµØ§Ø­ÙŠÙ‡ 3 Ø§ÙŠØ§Ù… Ùˆ Ø¨Ø­Ù„Ù… Ø¨ÙƒØ§Ø¨ÙˆØ³ Ø¨ÙŠØªÙƒØ±Ø± Ø¹Ù„ÙŠ Ø·ÙˆÙ„ Ø§Ù†ÙŠ Ø¨Ø§Ø°Ù‰ Ù†ÙØ³ÙŠ Ù„Ø¯Ø±Ø¬Ù‡ Ø§Ù†ÙŠ ÙÙƒØ±Øª ÙÙŠ ÙƒØ¯Ù‡	",
    "answer": "Ø£Ù†ØªÙŠ Ø¨Ø­Ø§Ø¬Ù‡ Ù„Ø²ÙŠØ§Ø±Ø© Ø·Ø¨ÙŠØ¨ Ù†ÙØ³ÙŠ Ù„Ù„ØªÙ‚ÙŠÙ… Ø§Ø®Ø° Ø§Ù„Ø³ÙŠØ±Ù‡ Ø§Ù„Ù…Ø±Ø¶ÙŠÙ‡ Ø¨Ø´ÙƒÙ„ Ø§Ø¯Ù‚ ÙˆÙ„Ù…Ø¹Ø±ÙØ© ØªÙØ§ØµÙŠÙ„ Ø§Ø®Ø±Ù‰ Ù…Ù‡Ù…Ù‡ ÙÙŠ Ø§Ù„ØªØ´Ø®ÙŠØµ Ù„Ø§ ØªØªØ±Ø¯Ø¯ÙŠ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ù‡ Ù„Ø§Ù† Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø±Ø¶ Ø§Ù„Ù†ÙØ³ÙŠ Ø§Ù„Ø­Ø§Ø¯ Ø¨Ø¯Ø§Øª ØªØ¸Ù‡Ø± Ø¨Ù‚Ù„Ù‡ Ø§Ù„Ø­Ø§Ø¬Ù‡ Ù„Ù„Ù†ÙˆÙ… ØªÙ…Ù†ÙŠØ§ØªÙŠ Ø¨Ø§Ù„Ø´ÙØ§Ø¡ Ø§Ù„Ø¹Ø§Ø¬Ù„	"
}

example_2 = {
    "question": "ØªÙ…Ù†Ø¹Ù†ÙŠ Ø¸Ø±ÙˆÙ Ù…Ø§ Ø§Ù‚Ø¯Ø± Ø§Ø±ÙˆØ­ Ø¯ÙƒØªÙˆØ± Ø§Ø®ØµØ§Ø¦ÙŠ ÙˆÙ„Ø§ Ø§Ø´Ø±Ø­ Ø­Ø§Ù„ØªÙŠ ÙƒØ¯Ø§Ù…Ù‡ ØµØ±Øª Ø§ÙÙƒØ± Ø¨Ø§Ù„Ø§Ù†ØªØ­Ø§Ø± Ø¨Ø³Ø¨Ø¨ Ù‡Ø§ÙŠ Ø§Ù„Ø­Ø§Ù„Ù‡ Ø´ÙŠ Ù…Ø®ÙŠÙ Ø£Ø´Ø¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¹Ø°Ø§Ø¨ Ù„Ø¯Ø±Ø¬Ù‡ Ø§Ù„Ù…ÙˆØª Ø£Ù‡ÙˆÙ† Ø§Ù„Ù…ÙˆØª Ø£Ù‡ÙˆÙ†	",
    "answer": "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø§Ø¯Ù…Ø§Ù† Ù…Ø­ØªØ§Ø¬ Ø·Ø¨ÙŠØ¨ ÙˆØ§Ø¯ÙˆÙŠØ© ÙˆØ§Ù„Ø§Ù‡Ù… ÙŠÙƒÙˆÙ† Ø¹Ù†Ø¯Ùƒ Ø§Ø±Ø§Ø¯Ù‡ Ù‚ÙˆÙŠÙ‡ Ù„Ù„ØªØ®Ù„Øµ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø³Ù…ÙˆÙ… ÙˆØ¹Ù„Ù‰ Ù‚Ø¯Ø± Ø¹Ø²ÙŠÙ…ØªÙƒ Ù‡ÙŠÙƒÙˆÙ† Ø§Ù„ØªÙˆÙÙŠÙ‚ Ù„Ù†Ø¬Ø§Ø­ Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø³Ø§Ø¹Ø¯ Ù†ÙØ³Ùƒ	"
}

example_3 = {
    "question": "Ø§Ø¹Ø§Ù†ÙŠ Ù…Ù† Ø§Ù„Ø®ÙˆÙ ÙˆØªÙ†Ø¨Ø¤Ø§Øª Ù…Ù† ÙƒÙ„Ø§Ù… Ø§Ù‡Ù„ÙŠ ÙˆØ¨ØµÙ…ØªÙ‡Ù… Ø§Ù†Ù‡Ù… ÙŠØ¨ÙˆÙ† ÙŠÙ‚ØªÙ„ÙˆÙ†ÙŠ Ø¨Ø¯ÙˆÙ† Ø³Ø¨Ø¨ ÙˆØ§Ù„ÙŠ Ù…Ø­Ø±Ø¶Ù‡Ù… Ø§Ø¨ÙˆÙŠ",
    "answer": "Ø§Ù„ØªÙ‚Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø²Ø§Ø¬ÙŠÙ‡ ÙÙŠ Ø³Ù†Ùƒ Ø§Ù„Ù…Ø¨ÙƒØ±Ù‡ ÙƒØ«ÙŠØ±Ù‡ ÙˆÙ…Ù† Ø§Ù„Ù…ÙÙŠØ¯ Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ø·Ø¨ÙŠØ¨ Ù„Ù„ØªØ´Ø®ÙŠØµ Ø§Ù„Ø¯Ù‚ÙŠÙ‚	"
}

def build_fewshot_prompt(new_question):
    prompt = f"""
Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ø§Ø­ØªØ±Ø§ÙÙŠØ© ÙˆØ¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:

Ø³: {example_1['question']}
Ø¬: {example_1['answer']}

Ø³: {example_2['question']}
Ø¬: {example_2['answer']}

Ø³: {example_3['question']}
Ø¬: {example_3['answer']}

Ø³: {new_question}
Ø¬:"""
    return prompt.strip()

def generate_llama_answer(question):
    prompt = build_fewshot_prompt(question)
    response = chat.invoke([
        HumanMessage(content=prompt)
    ])
    return response.content.strip()

sample_df = df[['question_clean_2', 'answer_clean_2']].sample(n=10, random_state=42).copy()
sample_df['llama_few_shot_answer'] = sample_df['question_clean_2'].apply(generate_llama_answer)
sample_df.to_csv("llama_few_shot_answer.csv", index=False)

from bert_score import score

P, R, F1 = score(
    sample_df['llama_few_shot_answer'].tolist(),
    sample_df['answer_clean_2'].tolist(),
    lang="ar",
    verbose=True
)

sample_df['bertscore_f1'] = F1.tolist()
sample_df.to_csv("llama_few_shot_answer.csv", index=False)
print(f"âœ… Average BERTScore F1: {F1.mean().item():.2f}")

import random

def generate_fewshot_sc_answer(question, n=10): # I traied 5 answers then 10 anwsers but the F1 is same
    answers = []
    for _ in range(n):
        prompt = build_fewshot_prompt(question)
        response = chat.invoke(prompt, temperature=0.1)
        answer = response.content.strip()
        answers.append(answer)

    most_common = Counter(answers).most_common(1)[0][0]
    return most_common

sample_df = df[['question_clean_2', 'answer_clean_2']].sample(n=10, random_state=42).copy()
sample_df['llama_fewshot_sc_answer'] = sample_df['question_clean_2'].apply(generate_fewshot_sc_answer)
sample_df.to_csv("llama_fewshot_sc_answer.csv", index=False)

from bert_score import score

P, R, F1 = score(
    sample_df['llama_fewshot_sc_answer'].tolist(),
    sample_df['answer_clean_2'].tolist(),
    lang="ar",
    verbose=True
)

sample_df['bertscore_f1'] = F1.tolist()
sample_df.to_csv("llama_fewshot_sc_answer.csv", index=False)
print(f"âœ… Average BERTScore F1: {F1.mean().item():.2f}")

def generate_multiple_llama_answers(question, n=5):
    answers = []
    for _ in range(n):
        prompt = build_fewshot_prompt(question)
        response = chat.invoke(prompt, temperature=0.1)
        answers.append(response.content.strip())
    return answers

def build_refinement_prompt(question, answers):
    joined_answers = "\n\n".join(
    [f"Answer {i+1}: {ans}" for i, ans in enumerate(answers)]
    )

    return f"""Question: {question}

    These are different answers to the same question:

    {joined_answers}

    Based on the previous answers, write an improved and ideal answer, taking into account accuracy and clarity:

Final answer:"""

sample_df = df[['question_clean_2', 'answer_clean_2']].sample(n=10, random_state=42).copy()
sample_df['llama_er_answer'] = sample_df['question_clean_2'].apply(
    lambda q: build_refinement_prompt(q, generate_multiple_llama_answers(q, n=5))
)
sample_df.to_csv("llama_er_answer.csv", index=False)

from bert_score import score

P, R, F1 = score(
    sample_df['llama_er_answer'].tolist(),
    sample_df['answer_clean_2'].tolist(),
    lang="ar",
    verbose=True
)

sample_df['bertscore_er_answer'] = F1.tolist()
sample_df.to_csv("llama_er_answer.csv", index=False)
print(f"âœ… Average BERTScore F1: {F1.mean().item():.2f}")

example_1 = {
    "question": "Ø¨Ù‚Ø§Ù„ÙŠ Ø§ÙƒØªØ± Ù…Ù† Ø§Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù…Ø´ Ø¨Ù†Ø§Ù… ÙˆÙ„Ø§ Ø¨Ø§ÙƒÙ„ Ùˆ Ù„Ùˆ Ù†Ù…Øª Ø³Ø§Ø¹Ù‡ ÙˆØ§Ø­Ø¯Ù‡ ÙÙŠ Ø§Ù„ÙŠÙˆÙ… Ø§ÙØ¶Ù„ ØµØ§Ø­ÙŠÙ‡ 3 Ø§ÙŠØ§Ù… Ùˆ Ø¨Ø­Ù„Ù… Ø¨ÙƒØ§Ø¨ÙˆØ³ Ø¨ÙŠØªÙƒØ±Ø± Ø¹Ù„ÙŠ Ø·ÙˆÙ„ Ø§Ù†ÙŠ Ø¨Ø§Ø°Ù‰ Ù†ÙØ³ÙŠ Ù„Ø¯Ø±Ø¬Ù‡ Ø§Ù†ÙŠ ÙÙƒØ±Øª ÙÙŠ ÙƒØ¯Ù‡	",
    "answer": "Ø£Ù†ØªÙŠ Ø¨Ø­Ø§Ø¬Ù‡ Ù„Ø²ÙŠØ§Ø±Ø© Ø·Ø¨ÙŠØ¨ Ù†ÙØ³ÙŠ Ù„Ù„ØªÙ‚ÙŠÙ… Ø§Ø®Ø° Ø§Ù„Ø³ÙŠØ±Ù‡ Ø§Ù„Ù…Ø±Ø¶ÙŠÙ‡ Ø¨Ø´ÙƒÙ„ Ø§Ø¯Ù‚ ÙˆÙ„Ù…Ø¹Ø±ÙØ© ØªÙØ§ØµÙŠÙ„ Ø§Ø®Ø±Ù‰ Ù…Ù‡Ù…Ù‡ ÙÙŠ Ø§Ù„ØªØ´Ø®ÙŠØµ Ù„Ø§ ØªØªØ±Ø¯Ø¯ÙŠ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ù‡ Ù„Ø§Ù† Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø±Ø¶ Ø§Ù„Ù†ÙØ³ÙŠ Ø§Ù„Ø­Ø§Ø¯ Ø¨Ø¯Ø§Øª ØªØ¸Ù‡Ø± Ø¨Ù‚Ù„Ù‡ Ø§Ù„Ø­Ø§Ø¬Ù‡ Ù„Ù„Ù†ÙˆÙ… ØªÙ…Ù†ÙŠØ§ØªÙŠ Ø¨Ø§Ù„Ø´ÙØ§Ø¡ Ø§Ù„Ø¹Ø§Ø¬Ù„	"
}

example_2 = {
    "question": "ØªÙ…Ù†Ø¹Ù†ÙŠ Ø¸Ø±ÙˆÙ Ù…Ø§ Ø§Ù‚Ø¯Ø± Ø§Ø±ÙˆØ­ Ø¯ÙƒØªÙˆØ± Ø§Ø®ØµØ§Ø¦ÙŠ ÙˆÙ„Ø§ Ø§Ø´Ø±Ø­ Ø­Ø§Ù„ØªÙŠ ÙƒØ¯Ø§Ù…Ù‡ ØµØ±Øª Ø§ÙÙƒØ± Ø¨Ø§Ù„Ø§Ù†ØªØ­Ø§Ø± Ø¨Ø³Ø¨Ø¨ Ù‡Ø§ÙŠ Ø§Ù„Ø­Ø§Ù„Ù‡ Ø´ÙŠ Ù…Ø®ÙŠÙ Ø£Ø´Ø¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¹Ø°Ø§Ø¨ Ù„Ø¯Ø±Ø¬Ù‡ Ø§Ù„Ù…ÙˆØª Ø£Ù‡ÙˆÙ† Ø§Ù„Ù…ÙˆØª Ø£Ù‡ÙˆÙ†	",
    "answer": "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø§Ø¯Ù…Ø§Ù† Ù…Ø­ØªØ§Ø¬ Ø·Ø¨ÙŠØ¨ ÙˆØ§Ø¯ÙˆÙŠØ© ÙˆØ§Ù„Ø§Ù‡Ù… ÙŠÙƒÙˆÙ† Ø¹Ù†Ø¯Ùƒ Ø§Ø±Ø§Ø¯Ù‡ Ù‚ÙˆÙŠÙ‡ Ù„Ù„ØªØ®Ù„Øµ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø³Ù…ÙˆÙ… ÙˆØ¹Ù„Ù‰ Ù‚Ø¯Ø± Ø¹Ø²ÙŠÙ…ØªÙƒ Ù‡ÙŠÙƒÙˆÙ† Ø§Ù„ØªÙˆÙÙŠÙ‚ Ù„Ù†Ø¬Ø§Ø­ Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø³Ø§Ø¹Ø¯ Ù†ÙØ³Ùƒ	"
}

example_3 = {
    "question": "Ø§Ø¹Ø§Ù†ÙŠ Ù…Ù† Ø§Ù„Ø®ÙˆÙ ÙˆØªÙ†Ø¨Ø¤Ø§Øª Ù…Ù† ÙƒÙ„Ø§Ù… Ø§Ù‡Ù„ÙŠ ÙˆØ¨ØµÙ…ØªÙ‡Ù… Ø§Ù†Ù‡Ù… ÙŠØ¨ÙˆÙ† ÙŠÙ‚ØªÙ„ÙˆÙ†ÙŠ Ø¨Ø¯ÙˆÙ† Ø³Ø¨Ø¨ ÙˆØ§Ù„ÙŠ Ù…Ø­Ø±Ø¶Ù‡Ù… Ø§Ø¨ÙˆÙŠ",
    "answer": "Ø§Ù„ØªÙ‚Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø²Ø§Ø¬ÙŠÙ‡ ÙÙŠ Ø³Ù†Ùƒ Ø§Ù„Ù…Ø¨ÙƒØ±Ù‡ ÙƒØ«ÙŠØ±Ù‡ ÙˆÙ…Ù† Ø§Ù„Ù…ÙÙŠØ¯ Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ø·Ø¨ÙŠØ¨ Ù„Ù„ØªØ´Ø®ÙŠØµ Ø§Ù„Ø¯Ù‚ÙŠÙ‚	"
}

example_4 = {
    "question": "Ø¹Ù…Ø±Ù‡Ø§ 15 Ù…Ù†Ø° Ù‚Ø±Ø§Ø¨Ù‡ Ø´Ù‡Ø±ÙŠÙ† Ø§Ùˆ Ø§ÙƒØ«Ø± Ø¨Ø¯Ø§Øª Ø­Ø§Ù„ØªÙ‡Ø§ ØªØªØºÙŠØ± Ù…Ù† Ø§Ù„Ù‡Ø¯ÙˆØ¡ Ø§Ù„Ù‰ Ø§Ù„Ø¹ØµØ¨ÙŠÙ‡ ÙˆØ¹Ø¯Ù… Ø§Ù„Ù†ÙˆÙ…",
    "answer": "Ù‡Ù†Ø§Ùƒ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø§Ø³Ø¦Ù„Ù‡ Ù‡Ù„ ÙŠÙˆØ¬Ø¯ Ø®Ù„Ø§ÙØ§Øª ÙÙŠ Ø§Ù„Ù…Ù†Ø²Ù„ Ù‡Ù„ ØªØ¹Ø±Ø¶Øª Ù„Ø§ÙŠ Ø¶ØºÙˆØ· Ø§Ùˆ Ø§ÙŠ ØµØ¯Ù…Ù‡ ØŸ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¨Ø§Ø´Ø±Ù‡ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬ Ù†ÙØ³ÙŠ Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ù‡"
}

example_5 = {
    "question": "Ù‚Ù„Ù‚ ÙˆØªÙˆØªØ± ÙˆÙˆØ³ÙˆØ§Ø³ Ù‚Ù‡Ø±ÙŠ ÙˆØªØ§ØªØ§Ù‡ ÙÙŠ Ø§Ù„ÙƒÙ„Ø§Ù… ÙˆØ¹Ø¬Ø² Ø¯Ø§Ø®Ù„ÙŠ Ø§Ø¸Ù† Ø§Ù†ÙŠ Ù…ÙƒØ±ÙˆÙ‡ ÙˆØ§Ù†ÙŠ Ù„Ø§ Ø§Ù‚Ø¯Ø± Ø§ÙØ¹Ù„ Ø´ÙŠ	",
    "answer": "Ø§Ù†Ø´ØºÙ„ Ø¨Ø§Ù„Ø¯Ø±Ø§Ø³Ù‡ Ø§ÙˆØ§Ù„Ø¹Ù…Ù„ Ø§Ùˆ Ø§Ù„Ø§Ù†Ø´Ø·Ù‡ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠÙ‡ ÙˆÙ…Ø¹ ØªÙ„Ù‚ÙŠ Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø§Ù„Ù†ÙØ³ÙŠ ØªØªØ­Ø³Ù† Ø§Ù„Ø­Ø§Ù„Ù‡ Ù…Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØµØ¨Ø±"
}

def build_fewshot_5_prompt(new_question):
    prompt = f"""
Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ø§Ø­ØªØ±Ø§ÙÙŠØ© ÙˆØ¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:

Ø³: {example_1['question']}
Ø¬: {example_1['answer']}

Ø³: {example_2['question']}
Ø¬: {example_2['answer']}

Ø³: {example_3['question']}
Ø¬: {example_3['answer']}

Ø³: {example_4['question']}
Ø¬: {example_4['answer']}

Ø³: {example_5['question']}
Ø¬: {example_5['answer']}

Ø³: {new_question}
Ø¬:"""
    return prompt.strip()

def generate_llama_fewshot_5_answer(question):
    prompt = build_fewshot_5_prompt(question)
    response = chat.invoke([
        HumanMessage(content=prompt)
    ])
    return response.content.strip()

sample_df = df[['question_clean_2', 'answer_clean_2']].sample(n=10, random_state=42).copy()
sample_df['llama_fewshot_5_answer'] = sample_df['question_clean_2'].apply(generate_llama_fewshot_5_answer)
sample_df.to_csv("llama_fewshot_5_answer.csv", index=False)

from bert_score import score

P, R, F1 = score(
    sample_df['llama_fewshot_5_answer'].tolist(),
    sample_df['answer_clean_2'].tolist(),
    lang="ar",
    verbose=True
)

sample_df['bertscore_fewshot_5'] = F1.tolist()
sample_df.to_csv("llama_fewshot_5_answer.csv", index=False)
print(f"âœ… 5 - Few-Shot BERTScore F1: {F1.mean().item():.2f}")

def build_fewshot_cot_prompt(new_question):
    prompt = f"""

Question: I can't sleep and feel stressed all day. Do I have a mental illness?

Let's analyze the condition gradually:
1. Look at the symptoms: insomnia (inability to sleep) and constant stress.
2. Associated with: disorders such as generalized anxiety or depression.
3. To properly diagnose the condition: You must know a few things.
4. To answer: Provide a professional answer that takes into account the above factors using one or more of the strategies:
- providing information
- Direct answers
- Emotional support

Answer: It is best to consult a psychologist to evaluate the condition. Lifestyle changes or behavioral therapy may help alleviate the symptoms.

Make sure your answer is fully formatted in Arabic and specifically designed to diagnose mental health conditions

Question: {new_question}
Answer:"""
    return prompt.strip()

def generate_llama_fewshot_cot_answer(question):
    prompt = build_fewshot_cot_prompt(question)
    response = chat.invoke([
        HumanMessage(content=prompt)
    ])
    return response.content.strip()

sample_df = df[['question_clean_2', 'answer_clean_2']].sample(n=10, random_state=42).copy()
sample_df['llama_fewshot_cot_answer'] = sample_df['question_clean_2'].apply(generate_llama_fewshot_cot_answer)
sample_df.to_csv("llama_fewshot_cot_answer.csv", index=False)

from bert_score import score

P, R, F1 = score(
    sample_df['llama_fewshot_cot_answer'].tolist(),
    sample_df['answer_clean_2'].tolist(),
    lang="ar",
    verbose=True
)

sample_df['bertscore_llama_fewshot_cot_answer'] = F1.tolist()
sample_df.to_csv("llama_fewshot_cot_answer.csv", index=False)
print(f"âœ… CoT BERTScore F1: {F1.mean().item():.2f}")

!pip install -U google-generativeai

from huggingface_hub import login
login()  # Ø£Ø¯Ø®Ù„ ØªÙˆÙƒÙ† HF

import google.generativeai as genai
print(f"Google Generative AI version: {genai.__version__}")

from huggingface_hub import __version__ as hf_version
print(f"Hugging Face Hub version: {hf_version}")

import sys
print(f"Python version: {sys.version}")

from transformers import AutoModelForCausalLM, AutoTokenizer

# Allam
allam_model = AutoModelForCausalLM.from_pretrained("ALLaM-AI/ALLaM-7B-Instruct-preview", torch_dtype="auto", device_map="auto")
allam_tokenizer = AutoTokenizer.from_pretrained("ALLaM-AI/ALLaM-7B-Instruct-preview")

# Qwen
qwen_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-7B-Instruct", torch_dtype="auto", device_map="auto")
qwen_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-7B-Instruct")

# FINAL aLLAm
def gen_hf(model, tokenizer, prompt):
    # 1. Tokenize the prompt and move to model's device
    toks = tokenizer(prompt, return_tensors="pt").to(model.device)

    # 2. Generate the model output
    out = model.generate(**toks, max_new_tokens=1024, temperature=0.1, do_sample=True, top_p=0.9)

    # 3. Decode only the generated part (exclude input prompt)
    generated_tokens = out[0][toks["input_ids"].shape[1]:]
    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)

    return response

# FINAL Qwen
def gen_qwen(model, tokenizer, prompt):
    # 1. Tokenize the prompt and move to model's device
    toks = tokenizer(prompt, return_tensors="pt").to(model.device)

    # 2. Generate output
    out = model.generate(**toks, max_new_tokens=1024, temperature=0.1, do_sample=True, top_p=0.9)

    # 3. Decode only the generated part
    generated_tokens = out[0][toks["input_ids"].shape[1]:]
    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)

    return response

# FINAL Gemini
import google.generativeai as genai
genai.configure(api_key="..")
gemini_model = genai.GenerativeModel("gemini-2.5-flash")

def gen_gemini(model, prompt, max_output_tokens=1024, temperature=0.1, top_p=0.9):
    response = model.generate_content(prompt)
    return response.text

#FINAL GPT

import openai

openai.api_key = "sk-proj...."
openai_model = "gpt-4o"

def gen_openai(prompt, model_name):
    response = openai.ChatCompletion.create(
        model=model_name,
        messages=[
            {"role": "user", "content": prompt}], max_tokens=1024,
                            temperature=0.1,
                            top_p=0.9
                            )
    return response.choices[0].message.content

# FINAL

import pandas as pd
from tqdm import tqdm
from bert_score import score

# Load 15 samples
#df = pd.read_csv("Train_Dev.tsv", sep="\t")
df_sample = df[['question_clean_2', 'answer_clean_2']].dropna().sample(n=20, random_state=42).reset_index(drop=True)

# Prepare all prompts
df_sample["prompt"] = df_sample["question_clean_2"].apply(build_fewshot_prompt)

# Initialize result containers
results = {
    "allam": [],
    "qwen": [],
    "gpt": [],
    "gemini": []
}

#FINAL

# Allam
for prompt in tqdm(df_sample["prompt"], desc="Allam"):
    ans = gen_hf(allam_model, allam_tokenizer, prompt)
    results["allam"].append(ans)

# Qwen
for prompt in tqdm(df_sample["prompt"], desc="Qwen"):
    ans = gen_qwen(qwen_model, qwen_tokenizer, prompt)
    results["qwen"].append(ans)

# OpenAI GPT
for prompt in tqdm(df_sample["prompt"], desc="GPT"):
    ans = gen_openai(prompt, openai_model)
    results["gpt"].append(ans)

# Gemini
for prompt in tqdm(df_sample["prompt"], desc="Gemini"):
    ans = gen_gemini(gemini_model, prompt)
    results["gemini"].append(ans)

bert_results = {}

for model_name in results.keys():
    print(f"Scoring {model_name}...")
    P, R, F1 = score(
        results[model_name],
        df_sample["answer_clean_2"].tolist(),
        lang="ar",
        verbose=True
    )
    bert_results[model_name] = {
        "precision": P.mean().item(),
        "recall": R.mean().item(),
        "f1": F1.mean().item()
    }

print("\nğŸ“Š Final Average BERTScore F1 per Model:")
for model_name, scores in bert_results.items():
    print(f"{model_name}: F1 = {scores['f1']:.4f}")