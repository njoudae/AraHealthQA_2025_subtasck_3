{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C39ZOTe1qS2"
      },
      "source": [
        "#  MentalQA Dataset Description\n",
        "\n",
        "The full dataset consists of **500 annotated samples**. For the purpose of this shared task, we are using only the **training and development set**:\n",
        "\n",
        "- **Train+Dev Set**: 350 samples  \n",
        "- **Test Set**: 150 samples (withheld for evaluation purposes)\n",
        "\n",
        "---\n",
        "\n",
        "###  Dataset File: `MentalQA_500_data.csv` (tab-separated)\n",
        "\n",
        "This dataset was recently accepted in the *IEEE ACCESS* journal.  \n",
        "It contains **four columns** as follows:\n",
        "\n",
        "1. **Question**  \n",
        "2. **Answer**  \n",
        "3. **Question Types** (`final_QT`)  \n",
        "4. **Answer Strategies** (`final_AS`)\n",
        "\n",
        "---\n",
        "\n",
        "###  Question Type Labels (`final_QT`)\n",
        "\n",
        "| Label | Name                       |Description |\n",
        "|----------|----------------------------|----------------\n",
        "| A        | **Diagnosis**              | Questions about clinical findings, tests, disease criteria, and manifestations. |\n",
        "| B        | **Treatment**              | Questions about therapies, drug use, side effects, and contraindications. |\n",
        "| C        | **Anatomy and Physiology** | Basic medical knowledge (e.g., tissues, organs, metabolism). |\n",
        "| D        | **Epidemiology**           | Disease progression, prognosis, etiology, and risk factors. |\n",
        "| E        | **Healthy Lifestyle**      | Diet, exercise, mood, and lifestyle factors affecting mental health. |\n",
        "| F        | **Provider Choices**       | Recommendations for doctors, hospitals, or medical departments. |\n",
        "| Z        | **Other**                  | General or uncategorized questions. |\n",
        "\n",
        "---\n",
        "\n",
        "###  Answer Strategy Labels (`final_AS`)\n",
        "\n",
        "| Label   | Name              | Description |\n",
        "|---------|-----------|-------------|\n",
        "| 1       | **Information**    | Provides factual data, explanations, or resources. |\n",
        "| 2       |  **Direct Guidance**| Offers suggestions, instructions, or behavioral advice. |\n",
        "| 3       | **Emotional Support** | Gives reassurance, empathy, or emotional encouragement. |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "GitHub: https://github.com/hasanhuz/MentalQA/tree/main\n",
        "\n",
        "Paper: https://arxiv.org/abs/2405.12619"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jnNX87LGz_Wc"
      },
      "outputs": [],
      "source": [
        "!pip install openai==0.28\n",
        "!pip install bert-score\n",
        "!pip install -U langchain langchain-community\n",
        "!pip install -U langchain-groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P91jTk812V0R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eamMs9_52ZzI"
      },
      "outputs": [],
      "source": [
        "pd.set_option(\"display.max_colwidth\", None, \"display.max_rows\", None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnCSAkcl2cZl"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/Train_Dev.tsv', sep='\\t')\n",
        "data.sample(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocqU1VES26NT"
      },
      "source": [
        "### Start Cleaning Dataset\n",
        "##  Data Preprocessing and Cleaning Overview\n",
        "\n",
        "Before performing any analysis or model evaluation, it is crucial to clean and normalize the dataset to ensure consistency and quality.\n",
        "\n",
        "This section outlines the preprocessing steps applied to the MentalQA Arabic dataset.\n",
        "\n",
        "---\n",
        "\n",
        "###  Cleaning Steps Applied:\n",
        "\n",
        "1. **Displayed full text columns** â€“ Enabled full visibility of long question/answer text using `pd.set_option(\"display.max_colwidth\", None)`.\n",
        "2. **Email Removal** â€“ Removed any embedded email addresses using regular expressions.\n",
        "3. **Separation Rules** â€“ Inserted spaces between:\n",
        "   - Arabic letters and numbers (e.g., \"Ø¯ÙˆØ§Ø¡20\" â†’ \"Ø¯ÙˆØ§Ø¡ 20\")\n",
        "   - Arabic and English terms (e.g., \"Ø¹Ù„Ø§Ø¬mg\" â†’ \"Ø¹Ù„Ø§Ø¬ mg\")\n",
        "   - Numbers and English words (e.g., \"mg75\" â†’ \"mg 75\")\n",
        "4. **Punctuation Cleanup** â€“ Removed all punctuation marks except the Arabic question mark \"ØŸ\".\n",
        "5. **Arabic Letter Normalization** â€“ Standardized variations of alef (Ø§), ta marbuta (Ù‡), and others.\n",
        "6. **Numeric Normalization** â€“ Converted Arabic-Indic numerals (Ù Ù¡Ù¢...) to Western digits (012...).\n",
        "7. **Repetition Reduction** â€“ Compressed elongated words (e.g., \"Ø±Ø§Ø§Ø§Ø§Ø§Ø¦Ø¹\" â†’ \"Ø±Ø§Ø¦Ø¹\").\n",
        "8. **Question Mark Spacing** â€“ Ensured space before and after \"ØŸ\" to avoid it sticking to adjacent words.\n",
        "9. **Custom Corrections** â€“ Applied multiple custom dictionaries to fix common spelling errors, typos, and fused terms (e.g., \"Ù…Ø§Ø§\" â†’ \"Ù…Ø§\", \"Ø¨Ù„Ù†ØªØ­Ø§Ø±\" â†’ \"Ø¨Ø§Ù„Ø§Ù†ØªØ­Ø§Ø±\").\n",
        "\n",
        "---\n",
        "\n",
        "###  Word-Level Error Detection via Frequency Analysis\n",
        "\n",
        "To further enhance text quality, we analyzed both the cleaned questions and answers at the word level:\n",
        "\n",
        "- Combined all tokens from `question_clean_1` and `answer_clean_1`.\n",
        "- Used `collections.Counter` to calculate word frequencies.\n",
        "- Created a sorted DataFrame to highlight rare and suspicious words.\n",
        "- Manually reviewed low-frequency words to detect:\n",
        "  - Typos (e.g., \"Ù‡Ø§Ø°Ø§\", \"Ù…Ø¹Ø§ÙŠØ§\", \"Ù…ÙØ§Ø±Ù‚Ù‡Ù…ØªØµÙ„\")\n",
        "  - Word merges and informal language\n",
        "- Built custom correction dictionaries based on the findings to normalize the vocabulary.\n",
        "\n",
        "This iterative refinement helped reduce vocabulary noise and improve input quality for downstream modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4XYMWRZ24oA"
      },
      "outputs": [],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUh5IcMC3FNr"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "y8FB5CT33HQO"
      },
      "outputs": [],
      "source": [
        "df = data[['question', 'answer']]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycO54aKj3PRJ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_arabic_text(text):\n",
        "    if pd.isnull(text):\n",
        "        return \"\"\n",
        "    # 0. Normalize numbers\n",
        "    text = text.translate(str.maketrans(\"Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©\", \"0123456789\"))\n",
        "\n",
        "    # 1. Remove Email (ÙˆØ§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡ Ø¨Ù…Ø³Ø§ÙØ©)\n",
        "    text = re.sub(r'\\S*@\\S+', ' ', text)\n",
        "\n",
        "    # 2. Make Distance between Arabic and Numbers/English\n",
        "    text = re.sub(r'([Ø¡-ÙŠ])(\\d+)', r'\\1 \\2', text)\n",
        "    text = re.sub(r'(\\d+)([Ø¡-ÙŠ])', r'\\1 \\2', text)\n",
        "    text = re.sub(r'([Ø¡-ÙŠ])([a-zA-Z]+)', r'\\1 \\2', text)\n",
        "    text = re.sub(r'([a-zA-Z]+)([Ø¡-ÙŠ])', r'\\1 \\2', text)\n",
        "    text = re.sub(r'([a-zA-Z]+)(\\d+)', r'\\1 \\2', text)\n",
        "    text = re.sub(r'(\\d+)([a-zA-Z]+)', r'\\1 \\2', text)\n",
        "\n",
        "    # 3. Remove repeated characters (Ù…Ø«Ù„Ø§Ù‹: ÙƒÙŠÙŠÙŠÙŠÙ â†’ ÙƒÙŠÙ)\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
        "\n",
        "    # 4. Normalize Arabic letters\n",
        "    text = re.sub(r'[Ø¥Ø£Ø¢Ø§]', 'Ø§', text)\n",
        "    text = re.sub(r'Ø©', 'Ù‡', text)\n",
        "    text = re.sub(r'Ú¯', 'Ùƒ', text)\n",
        "\n",
        "    # 5. Remove punctuation (except ØŸ) Ù…Ø¹ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡Ø§ Ø¨Ù…Ø³Ø§ÙØ©\n",
        "    text = re.sub(r'[^\\w\\sØŸ]', ' ', text)\n",
        "\n",
        "    # 6. Ensure space before and after question mark\n",
        "    text = re.sub(r'\\s*ØŸ\\s*', ' ØŸ ', text)\n",
        "\n",
        "    # 7. Remove extra whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "df[\"question_clean_1\"] = df[\"question\"].apply(clean_arabic_text)\n",
        "df[\"answer_clean_1\"] = df[\"answer\"].apply(clean_arabic_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KrrDIsH_3aqn"
      },
      "outputs": [],
      "source": [
        "all_text = ' '.join(df['question_clean_1'].astype(str))\n",
        "words = all_text.split()\n",
        "word_freq = Counter(words)\n",
        "freq_df = pd.DataFrame(word_freq.items(), columns=['word', 'count']).sort_values(by='count', ascending=True)\n",
        "freq_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rItI52T43dwm"
      },
      "outputs": [],
      "source": [
        "def apply_custom_corrections(text):\n",
        "    corrections = {\n",
        "        \"Ù‚ÙŠ\": \"ÙÙŠ\",\n",
        "        \"Ø§Ù†Ø§Ø§Ø°ÙƒØ±\": \"Ø£Ù†Ø§ Ø£Ø°ÙƒØ±\",\n",
        "        \"Ù…Ø§Ø§\": \"Ù…Ø§\",\n",
        "        \"Ø¹Ù„ÙŠÙƒÙ…Ø§Ø¹Ø§Ù†ÙŠ\": \"Ø¹Ù„ÙŠÙƒÙ… Ø§Ø¹Ø§Ù†ÙŠ\",\n",
        "        \"Ù‡Ø¯Ù‡\": \"Ù‡Ø°Ù‡\",\n",
        "        \"Ù„Ù„Ø§ÙƒØªØ¦Ø§\": \"Ù„Ù„Ø§ÙƒØªØ¦Ø§Ø¨\",\n",
        "        \"Ø¨Ù„Ø³ÙˆØ¡\": \"Ø¨ Ø§Ù„Ø³ÙˆØ¡\",\n",
        "        \"Ø¡Ø§Ø°ÙŠ\": \"Ø§Ù„Ø°ÙŠ\",\n",
        "        \"Ù…Ø§ÙƒØ¯Ø±\": \"Ù…Ø§ Ø§Ù‚Ø¯Ø±\",\n",
        "        \"Ù…Ø¹Ø§ÙŠØ§\": \"Ù…Ø¹ÙŠ\",\n",
        "        \"Ø­ØªØ§\": \"Ø­ØªÙ‰\",\n",
        "        \"Ø§Ù†ÙˆØ§\": \"Ø£Ù†Ù‡\",\n",
        "        \"Ø§Ø®Ø±ÙŠ\": \"Ø£Ø®Ø±Ù‰\",\n",
        "        \"Ø¨Ù„Ù†ØªØ­Ø§Ø±\": \"Ø¨ Ø§Ù„Ø§Ù†ØªØ­Ø§Ø±\",\n",
        "        \"Ø¨Ù„Ø§Ø­Ø¨Ø§Ø·\": \"Ø¨ Ø§Ù„Ø¥Ø­Ø¨Ø§Ø·\",\n",
        "        \"Ù…Ù†Ù‡Ø§Ø§\": \"Ù…Ù†Ù‡Ø§\",\n",
        "        \"Ø§Ø¨ØºØ§\": \"Ø£Ø±ÙŠØ¯\",\n",
        "        \"Ø·Ù…Ø§Ù†ØªÙ†ÙŠ\": \"Ø·Ù…Ù†ØªÙ†ÙŠ\",\n",
        "        \"ÙˆÙˆØ§Ø­Ø¨Ù‡Ø§\": \"ÙˆØ§Ø­Ø¨Ù‡Ø§\",\n",
        "        \"Ù…ÙØ¹ÙˆÙ„Ø®\": \"Ù…ÙØ¹ÙˆÙ„Ù‡\",\n",
        "\n",
        "    }\n",
        "\n",
        "    for wrong, right in corrections.items():\n",
        "        text = text.replace(wrong, right)\n",
        "    return text\n",
        "\n",
        "df[\"question_clean_2\"] = df[\"question_clean_1\"].apply(apply_custom_corrections)\n",
        "#df[\"answer_clean_2\"] = df[\"answer_clean_1\"].apply(clean_arabic_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "m4utnIb33hzO"
      },
      "outputs": [],
      "source": [
        "all_text_ = ' '.join(df['answer_clean_1'].astype(str))\n",
        "words_ = all_text_.split()\n",
        "word_freq_ = Counter(words_)\n",
        "freq_df_ = pd.DataFrame(word_freq_.items(), columns=['word', 'count']).sort_values(by='count', ascending=True)\n",
        "freq_df_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raOH0zW53jPe"
      },
      "outputs": [],
      "source": [
        "def apply_answer_corrections(text):\n",
        "    corrections = {\n",
        "        \"Ø§Ù„Ù†ÙØ³ÙŠÙˆØ±Ø¨Ù…Ø§\": \"Ø§Ù„Ù†ÙØ³ÙŠ ÙˆØ±Ø¨Ù…Ø§\",\n",
        "        \"Ø§Ù„Ø¹Ù‚Ø§Ù‚ÙŠØ±ÙˆÙŠØ³ØªØ­Ø³Ù†\": \"Ø§Ù„Ø¹Ù‚Ø§Ù‚ÙŠØ± ÙˆÙŠØ³ØªØ­Ø³Ù†\",\n",
        "        \"ÙˆØ¶Ø±ÙˆØ±ÙŠÙˆØ§ÙŠØ°Ø§Ø¡\": \"Ùˆ Ø¶Ø±ÙˆØ±ÙŠ Ùˆ Ø¥ÙŠØ°Ø§Ø¡\",\n",
        "        \"ÙˆØ§Ù„Ø¬Ø±Ø¹Ù‡Ù„ÙŠÙ‚Ø¯Ø±Ù‡Ø§\": \"Ùˆ Ø§Ù„Ø¬Ø±Ø¹Ø© Ù„ÙŠÙ‚Ø¯Ø±Ù‡Ø§\",\n",
        "        \"ØªØ³ÙˆØ¡ÙˆØ±Ø¨Ù…Ø§\": \"ØªØ³ÙˆØ¡ ÙˆØ±Ø¨Ù…Ø§\",\n",
        "        \"ØªØºÙŠÙŠØ±Ø§Ùˆ\": \"ØªØºÙŠÙŠØ± Ø£Ùˆ\",\n",
        "        \"Ø§Ø¨Ù‚\": \"Ø£Ø¨Ù‚Ù‰\",\n",
        "        \"Ø§Ù„Ø§Ø¯ÙˆÙŠÙ‡Ù†Ø­ØªØ§Ø¬\": \"Ø§Ù„Ø£Ø¯ÙˆÙŠØ© Ù†Ø­ØªØ§Ø¬\",\n",
        "        \"Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠÙ‡Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯\": \"Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ÙŠØ© Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯\",\n",
        "        \"Ø§Ù„Ø­Ù„ØªØ¯Ø±ÙŠØ¬ÙŠØ§\": \"Ø§Ù„Ø­Ù„ ØªØ¯Ø±ÙŠØ¬Ø§\",\n",
        "        \"Ù…ÙØ§Ø±Ù‚Ù‡Ø§Ù„ØªØ¹Ù„Ù‚\": \"Ù…ÙØ§Ø±Ù‚Ø© Ø§Ù„ØªØ¹Ù„Ù‚\",\n",
        "        \"Ø®Ø·ÙŠØ±Ù‡Ù„Ø§Ù†Ù‡Ø§\": \"Ø®Ø·ÙŠØ±Ø© Ù„Ø£Ù†Ù‡Ø§\",\n",
        "        \"Ù†ÙØ³ÙŠØ§Ù„Ø§ÙÙƒØ§Ø±Ø§Ø§Ù„Ø§Ù†ØªØ­Ø§Ø±ÙŠÙ‡\": \"Ù†ÙØ³ÙŠ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø§Ù†ØªØ­Ø§Ø±ÙŠØ©\",\n",
        "        \"Ø§Ù„Ø®Ø¶Ø§Ø±Ù‚Ù„Ù„ÙŠ\": \"Ø§Ù„Ø®Ø¶Ø§Ø± Ù‚Ù„Ù„ÙŠ\",\n",
        "        \"ØºÙ„Ù‰\": \"Ø¹Ù„Ù‰\",\n",
        "        \"Ø§Ù„Ø¬ØªÙ…Ø§Ø¹ÙŠ\": \"Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ\",\n",
        "        \"Ù„ÙƒÙ„ÙŠÙ‡Ù…Ø§ÙˆÙ†ØªÙˆÙ‚Ù\": \"Ù„ÙƒÙ„ÙŠÙ‡Ù…Ø§ Ùˆ Ù†ØªÙˆÙ‚Ù\",\n",
        "        \"ÙˆØ§Ù†ÙˆØ§Ø¹Ù‡Ø§ÙˆÙ„ÙˆÙ…\": \"Ùˆ Ø£Ù†ÙˆØ§Ø¹Ù‡Ø§ Ùˆ Ù„ÙˆÙ…\",\n",
        "        \"Ø§Ù„Ù†Ø§Ø³Ù†Ø­ØªØ§Ø¬\": \"Ø§Ù„Ù†Ø§Ø³ Ù†Ø­ØªØ§Ø¬\",\n",
        "        \"Ø§Ù„Ù…Ø±Ø§Ø¡Ù‡\": \"Ø§Ù„Ù…Ø±Ø§Ø©\",\n",
        "        \"Ù‡Ø§Ø°Ø§\": \"Ù‡Ø°Ø§\",\n",
        "        \"Ø§Ù„Ù…Ø³Ø³Ø¨Ù‡\": \"Ø§Ù„Ù…Ø³Ø¨Ø¨Ù‡\",\n",
        "        \"Ø§Ù‰\": \"Ø§ÙŠ\",\n",
        "        \"Ø¹Ø¶ÙˆÙ‰\": \"Ø¹Ø¶ÙˆÙŠ\",\n",
        "        \"Ø§Ù„Ù†ÙØ³Ø§Ù†ÙŠØ§Ù„Ø§ÙÙƒØ§Ø±\": \"Ø§Ù„Ù†ÙØ³Ø§Ù†ÙŠ Ø§Ù„Ø£ÙÙƒØ§Ø±\",\n",
        "        \"Ù‡Ø¯Ø§\": \"Ù‡Ø°Ø§\",\n",
        "        \"Ø§Ø³Ø±Øº\": \"Ø§Ø³Ø±Ø¹\",\n",
        "        \"ÙˆØ§Ù„Ø§Ø³ØªØ±Ø®Ø§Ø¡ÙˆÙ‡Ø°Ø§\": \"Ùˆ Ø§Ù„Ø§Ø³ØªØ±Ø®Ø§Ø¡ Ùˆ Ù‡Ø°Ø§\",\n",
        "        \"Ø§Ù„Ø§Ø®Ø±ÙØªØµÙ„\": \"Ø§Ù„Ø§Ø®Ø± ÙØªØµÙ„\",\n",
        "        \"Ø§Ù„Ø§Ø®Ø±ÙˆØ¹Ù†Ø¯Ù…Ø§\": \"Ø§Ù„Ø§Ø®Ø± Ùˆ Ø¹Ù†Ø¯Ù…Ø§\",\n",
        "        \"Ø¯ÙˆØ§Ø¡ÙŠ\": \"Ø¯ÙˆØ§Ø¦ÙŠ\",\n",
        "        \"Ù…Ø´Ù‡ÙˆØ±Ù‡Ø§Ø³Ù…Ù‡Ø§\": \"Ù…Ø´Ù‡ÙˆØ± Ø§Ø³Ù…Ù‡\"\n",
        "    }\n",
        "\n",
        "    for wrong, right in corrections.items():\n",
        "        text = text.replace(wrong, right)\n",
        "    return text\n",
        "\n",
        "df[\"answer_clean_2\"] = df[\"answer_clean_1\"].apply(apply_answer_corrections)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8Mo4LucI3qrt"
      },
      "outputs": [],
      "source": [
        "# Cleaning Columns\n",
        "df[['question_clean_2', 'answer_clean_2']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpF3u6j56gMQ"
      },
      "outputs": [],
      "source": [
        "GROQ_API_KEY=\"gsk_....\"\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "\n",
        "chat = ChatGroq(\n",
        "    groq_api_key=GROQ_API_KEY,\n",
        "    model_name=\"llama3-70b-8192\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=1024\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWRKGewi8Qkz"
      },
      "outputs": [],
      "source": [
        "def build_zero_shot_prompt(question):\n",
        "    return f\"Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø§Ø­ØªØ±Ø§ÙÙŠØ© ÙˆØ¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:\\n\\nØ§Ù„Ø³Ø¤Ø§Ù„: {question}\\n\\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©:\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6D9yjjek8SKn"
      },
      "outputs": [],
      "source": [
        "def generate_llama_answer(question):\n",
        "    prompt = build_zero_shot_prompt(question)\n",
        "    response = chat.invoke([\n",
        "        HumanMessage(content=prompt)\n",
        "    ])\n",
        "    return response.content.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVe7W9r48T4D"
      },
      "outputs": [],
      "source": [
        "sample_df = df[['question_clean_2', 'answer_clean_2']].sample(n=10, random_state=42).copy()\n",
        "sample_df['llama_zero_shot_answer'] = sample_df['question_clean_2'].apply(generate_llama_answer)\n",
        "sample_df.to_csv(\"llama3_zero_shot_generated_answers.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYYziLpX8Wbs"
      },
      "outputs": [],
      "source": [
        "from bert_score import score\n",
        "\n",
        "P, R, F1 = score(\n",
        "    sample_df['llama_zero_shot_answer'].tolist(),\n",
        "    sample_df['answer_clean_2'].tolist(),\n",
        "    lang=\"ar\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "sample_df['bertscore_f1'] = F1.tolist()\n",
        "sample_df.to_csv(\"llama3_zero_shot_generated_answers.csv\", index=False)\n",
        "print(f\"âœ… Average BERTScore F1: {F1.mean().item():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUkQD8Tt4CTL"
      },
      "outputs": [],
      "source": [
        "example_1 = {\n",
        "    \"question\": \"Ø¨Ù‚Ø§Ù„ÙŠ Ø§ÙƒØªØ± Ù…Ù† Ø§Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù…Ø´ Ø¨Ù†Ø§Ù… ÙˆÙ„Ø§ Ø¨Ø§ÙƒÙ„ Ùˆ Ù„Ùˆ Ù†Ù…Øª Ø³Ø§Ø¹Ù‡ ÙˆØ§Ø­Ø¯Ù‡ ÙÙŠ Ø§Ù„ÙŠÙˆÙ… Ø§ÙØ¶Ù„ ØµØ§Ø­ÙŠÙ‡ 3 Ø§ÙŠØ§Ù… Ùˆ Ø¨Ø­Ù„Ù… Ø¨ÙƒØ§Ø¨ÙˆØ³ Ø¨ÙŠØªÙƒØ±Ø± Ø¹Ù„ÙŠ Ø·ÙˆÙ„ Ø§Ù†ÙŠ Ø¨Ø§Ø°Ù‰ Ù†ÙØ³ÙŠ Ù„Ø¯Ø±Ø¬Ù‡ Ø§Ù†ÙŠ ÙÙƒØ±Øª ÙÙŠ ÙƒØ¯Ù‡\t\",\n",
        "    \"answer\": \"Ø£Ù†ØªÙŠ Ø¨Ø­Ø§Ø¬Ù‡ Ù„Ø²ÙŠØ§Ø±Ø© Ø·Ø¨ÙŠØ¨ Ù†ÙØ³ÙŠ Ù„Ù„ØªÙ‚ÙŠÙ… Ø§Ø®Ø° Ø§Ù„Ø³ÙŠØ±Ù‡ Ø§Ù„Ù…Ø±Ø¶ÙŠÙ‡ Ø¨Ø´ÙƒÙ„ Ø§Ø¯Ù‚ ÙˆÙ„Ù…Ø¹Ø±ÙØ© ØªÙØ§ØµÙŠÙ„ Ø§Ø®Ø±Ù‰ Ù…Ù‡Ù…Ù‡ ÙÙŠ Ø§Ù„ØªØ´Ø®ÙŠØµ Ù„Ø§ ØªØªØ±Ø¯Ø¯ÙŠ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ù‡ Ù„Ø§Ù† Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø±Ø¶ Ø§Ù„Ù†ÙØ³ÙŠ Ø§Ù„Ø­Ø§Ø¯ Ø¨Ø¯Ø§Øª ØªØ¸Ù‡Ø± Ø¨Ù‚Ù„Ù‡ Ø§Ù„Ø­Ø§Ø¬Ù‡ Ù„Ù„Ù†ÙˆÙ… ØªÙ…Ù†ÙŠØ§ØªÙŠ Ø¨Ø§Ù„Ø´ÙØ§Ø¡ Ø§Ù„Ø¹Ø§Ø¬Ù„\t\"\n",
        "}\n",
        "\n",
        "example_2 = {\n",
        "    \"question\": \"ØªÙ…Ù†Ø¹Ù†ÙŠ Ø¸Ø±ÙˆÙ Ù…Ø§ Ø§Ù‚Ø¯Ø± Ø§Ø±ÙˆØ­ Ø¯ÙƒØªÙˆØ± Ø§Ø®ØµØ§Ø¦ÙŠ ÙˆÙ„Ø§ Ø§Ø´Ø±Ø­ Ø­Ø§Ù„ØªÙŠ ÙƒØ¯Ø§Ù…Ù‡ ØµØ±Øª Ø§ÙÙƒØ± Ø¨Ø§Ù„Ø§Ù†ØªØ­Ø§Ø± Ø¨Ø³Ø¨Ø¨ Ù‡Ø§ÙŠ Ø§Ù„Ø­Ø§Ù„Ù‡ Ø´ÙŠ Ù…Ø®ÙŠÙ Ø£Ø´Ø¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¹Ø°Ø§Ø¨ Ù„Ø¯Ø±Ø¬Ù‡ Ø§Ù„Ù…ÙˆØª Ø£Ù‡ÙˆÙ† Ø§Ù„Ù…ÙˆØª Ø£Ù‡ÙˆÙ†\t\",\n",
        "    \"answer\": \"Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø§Ø¯Ù…Ø§Ù† Ù…Ø­ØªØ§Ø¬ Ø·Ø¨ÙŠØ¨ ÙˆØ§Ø¯ÙˆÙŠØ© ÙˆØ§Ù„Ø§Ù‡Ù… ÙŠÙƒÙˆÙ† Ø¹Ù†Ø¯Ùƒ Ø§Ø±Ø§Ø¯Ù‡ Ù‚ÙˆÙŠÙ‡ Ù„Ù„ØªØ®Ù„Øµ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø³Ù…ÙˆÙ… ÙˆØ¹Ù„Ù‰ Ù‚Ø¯Ø± Ø¹Ø²ÙŠÙ…ØªÙƒ Ù‡ÙŠÙƒÙˆÙ† Ø§Ù„ØªÙˆÙÙŠÙ‚ Ù„Ù†Ø¬Ø§Ø­ Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø³Ø§Ø¹Ø¯ Ù†ÙØ³Ùƒ\t\"\n",
        "}\n",
        "\n",
        "example_3 = {\n",
        "    \"question\": \"Ø§Ø¹Ø§Ù†ÙŠ Ù…Ù† Ø§Ù„Ø®ÙˆÙ ÙˆØªÙ†Ø¨Ø¤Ø§Øª Ù…Ù† ÙƒÙ„Ø§Ù… Ø§Ù‡Ù„ÙŠ ÙˆØ¨ØµÙ…ØªÙ‡Ù… Ø§Ù†Ù‡Ù… ÙŠØ¨ÙˆÙ† ÙŠÙ‚ØªÙ„ÙˆÙ†ÙŠ Ø¨Ø¯ÙˆÙ† Ø³Ø¨Ø¨ ÙˆØ§Ù„ÙŠ Ù…Ø­Ø±Ø¶Ù‡Ù… Ø§Ø¨ÙˆÙŠ\",\n",
        "    \"answer\": \"Ø§Ù„ØªÙ‚Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø²Ø§Ø¬ÙŠÙ‡ ÙÙŠ Ø³Ù†Ùƒ Ø§Ù„Ù…Ø¨ÙƒØ±Ù‡ ÙƒØ«ÙŠØ±Ù‡ ÙˆÙ…Ù† Ø§Ù„Ù…ÙÙŠØ¯ Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ø·Ø¨ÙŠØ¨ Ù„Ù„ØªØ´Ø®ÙŠØµ Ø§Ù„Ø¯Ù‚ÙŠÙ‚\t\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l13L-X824EEs"
      },
      "outputs": [],
      "source": [
        "def build_fewshot_prompt(new_question):\n",
        "    prompt = f\"\"\"\n",
        "Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ø§Ø­ØªØ±Ø§ÙÙŠØ© ÙˆØ¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:\n",
        "\n",
        "Ø³: {example_1['question']}\n",
        "Ø¬: {example_1['answer']}\n",
        "\n",
        "Ø³: {example_2['question']}\n",
        "Ø¬: {example_2['answer']}\n",
        "\n",
        "Ø³: {example_3['question']}\n",
        "Ø¬: {example_3['answer']}\n",
        "\n",
        "Ø³: {new_question}\n",
        "Ø¬:\"\"\"\n",
        "    return prompt.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYNwU7d_4GHp"
      },
      "outputs": [],
      "source": [
        "def generate_llama_answer(question):\n",
        "    prompt = build_fewshot_prompt(question)\n",
        "    response = chat.invoke([\n",
        "        HumanMessage(content=prompt)\n",
        "    ])\n",
        "    return response.content.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnXCZtO99dSa"
      },
      "outputs": [],
      "source": [
        "sample_df = df[['question_clean_2', 'answer_clean_2']].sample(n=10, random_state=42).copy()\n",
        "sample_df['llama_few_shot_answer'] = sample_df['question_clean_2'].apply(generate_llama_answer)\n",
        "sample_df.to_csv(\"llama_few_shot_answer.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq9BFx8Y9k4j"
      },
      "outputs": [],
      "source": [
        "from bert_score import score\n",
        "\n",
        "P, R, F1 = score(\n",
        "    sample_df['llama_few_shot_answer'].tolist(),\n",
        "    sample_df['answer_clean_2'].tolist(),\n",
        "    lang=\"ar\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "sample_df['bertscore_f1'] = F1.tolist()\n",
        "sample_df.to_csv(\"llama_few_shot_answer.csv\", index=False)\n",
        "print(f\"âœ… Average BERTScore F1: {F1.mean().item():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SNdw4NJ-Wv1"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def generate_fewshot_sc_answer(question, n=10): # I traied 5 answers then 10 anwsers but the F1 is same\n",
        "    answers = []\n",
        "    for _ in range(n):\n",
        "        prompt = build_fewshot_prompt(question)\n",
        "        response = chat.invoke(prompt, temperature=0.1)\n",
        "        answer = response.content.strip()\n",
        "        answers.append(answer)\n",
        "\n",
        "    most_common = Counter(answers).most_common(1)[0][0]\n",
        "    return most_common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCsj-vaa-Z9H"
      },
      "outputs": [],
      "source": [
        "sample_df = df[['question_clean_2', 'answer_clean_2']].sample(n=10, random_state=42).copy()\n",
        "sample_df['llama_fewshot_sc_answer'] = sample_df['question_clean_2'].apply(generate_fewshot_sc_answer)\n",
        "sample_df.to_csv(\"llama_fewshot_sc_answer.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81YVC7ke-gpF"
      },
      "outputs": [],
      "source": [
        "from bert_score import score\n",
        "\n",
        "P, R, F1 = score(\n",
        "    sample_df['llama_fewshot_sc_answer'].tolist(),\n",
        "    sample_df['answer_clean_2'].tolist(),\n",
        "    lang=\"ar\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "sample_df['bertscore_f1'] = F1.tolist()\n",
        "sample_df.to_csv(\"llama_fewshot_sc_answer.csv\", index=False)\n",
        "print(f\"âœ… Average BERTScore F1: {F1.mean().item():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzroTRlI_NEy"
      },
      "outputs": [],
      "source": [
        "def generate_multiple_llama_answers(question, n=5):\n",
        "    answers = []\n",
        "    for _ in range(n):\n",
        "        prompt = build_fewshot_prompt(question)\n",
        "        response = chat.invoke(prompt, temperature=0.1)\n",
        "        answers.append(response.content.strip())\n",
        "    return answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5AnUNbg_QgH"
      },
      "outputs": [],
      "source": [
        "def build_refinement_prompt(question, answers):\n",
        "    joined_answers = \"\\n\\n\".join(\n",
        "    [f\"Answer {i+1}: {ans}\" for i, ans in enumerate(answers)]\n",
        "    )\n",
        "\n",
        "    return f\"\"\"Question: {question}\n",
        "\n",
        "    These are different answers to the same question:\n",
        "\n",
        "    {joined_answers}\n",
        "\n",
        "    Based on the previous answers, write an improved and ideal answer, taking into account accuracy and clarity:\n",
        "\n",
        "Final answer:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf8GIa5F_WhR"
      },
      "outputs": [],
      "source": [
        "sample_df = df[['question_clean_2', 'answer_clean_2']].sample(n=10, random_state=42).copy()\n",
        "sample_df['llama_er_answer'] = sample_df['question_clean_2'].apply(\n",
        "    lambda q: build_refinement_prompt(q, generate_multiple_llama_answers(q, n=5))\n",
        ")\n",
        "sample_df.to_csv(\"llama_er_answer.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5SaV2MN_bgB"
      },
      "outputs": [],
      "source": [
        "from bert_score import score\n",
        "\n",
        "P, R, F1 = score(\n",
        "    sample_df['llama_er_answer'].tolist(),\n",
        "    sample_df['answer_clean_2'].tolist(),\n",
        "    lang=\"ar\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "sample_df['bertscore_er_answer'] = F1.tolist()\n",
        "sample_df.to_csv(\"llama_er_answer.csv\", index=False)\n",
        "print(f\"âœ… Average BERTScore F1: {F1.mean().item():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRaPz_8s1py7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFNl6UQ0BB5V"
      },
      "outputs": [],
      "source": [
        "example_1 = {\n",
        "    \"question\": \"Ø¨Ù‚Ø§Ù„ÙŠ Ø§ÙƒØªØ± Ù…Ù† Ø§Ø³Ø¨ÙˆØ¹ÙŠÙ† Ù…Ø´ Ø¨Ù†Ø§Ù… ÙˆÙ„Ø§ Ø¨Ø§ÙƒÙ„ Ùˆ Ù„Ùˆ Ù†Ù…Øª Ø³Ø§Ø¹Ù‡ ÙˆØ§Ø­Ø¯Ù‡ ÙÙŠ Ø§Ù„ÙŠÙˆÙ… Ø§ÙØ¶Ù„ ØµØ§Ø­ÙŠÙ‡ 3 Ø§ÙŠØ§Ù… Ùˆ Ø¨Ø­Ù„Ù… Ø¨ÙƒØ§Ø¨ÙˆØ³ Ø¨ÙŠØªÙƒØ±Ø± Ø¹Ù„ÙŠ Ø·ÙˆÙ„ Ø§Ù†ÙŠ Ø¨Ø§Ø°Ù‰ Ù†ÙØ³ÙŠ Ù„Ø¯Ø±Ø¬Ù‡ Ø§Ù†ÙŠ ÙÙƒØ±Øª ÙÙŠ ÙƒØ¯Ù‡\t\",\n",
        "    \"answer\": \"Ø£Ù†ØªÙŠ Ø¨Ø­Ø§Ø¬Ù‡ Ù„Ø²ÙŠØ§Ø±Ø© Ø·Ø¨ÙŠØ¨ Ù†ÙØ³ÙŠ Ù„Ù„ØªÙ‚ÙŠÙ… Ø§Ø®Ø° Ø§Ù„Ø³ÙŠØ±Ù‡ Ø§Ù„Ù…Ø±Ø¶ÙŠÙ‡ Ø¨Ø´ÙƒÙ„ Ø§Ø¯Ù‚ ÙˆÙ„Ù…Ø¹Ø±ÙØ© ØªÙØ§ØµÙŠÙ„ Ø§Ø®Ø±Ù‰ Ù…Ù‡Ù…Ù‡ ÙÙŠ Ø§Ù„ØªØ´Ø®ÙŠØµ Ù„Ø§ ØªØªØ±Ø¯Ø¯ÙŠ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ´Ø§Ø±Ù‡ Ù„Ø§Ù† Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø±Ø¶ Ø§Ù„Ù†ÙØ³ÙŠ Ø§Ù„Ø­Ø§Ø¯ Ø¨Ø¯Ø§Øª ØªØ¸Ù‡Ø± Ø¨Ù‚Ù„Ù‡ Ø§Ù„Ø­Ø§Ø¬Ù‡ Ù„Ù„Ù†ÙˆÙ… ØªÙ…Ù†ÙŠØ§ØªÙŠ Ø¨Ø§Ù„Ø´ÙØ§Ø¡ Ø§Ù„Ø¹Ø§Ø¬Ù„\t\"\n",
        "}\n",
        "\n",
        "example_2 = {\n",
        "    \"question\": \"ØªÙ…Ù†Ø¹Ù†ÙŠ Ø¸Ø±ÙˆÙ Ù…Ø§ Ø§Ù‚Ø¯Ø± Ø§Ø±ÙˆØ­ Ø¯ÙƒØªÙˆØ± Ø§Ø®ØµØ§Ø¦ÙŠ ÙˆÙ„Ø§ Ø§Ø´Ø±Ø­ Ø­Ø§Ù„ØªÙŠ ÙƒØ¯Ø§Ù…Ù‡ ØµØ±Øª Ø§ÙÙƒØ± Ø¨Ø§Ù„Ø§Ù†ØªØ­Ø§Ø± Ø¨Ø³Ø¨Ø¨ Ù‡Ø§ÙŠ Ø§Ù„Ø­Ø§Ù„Ù‡ Ø´ÙŠ Ù…Ø®ÙŠÙ Ø£Ø´Ø¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¹Ø°Ø§Ø¨ Ù„Ø¯Ø±Ø¬Ù‡ Ø§Ù„Ù…ÙˆØª Ø£Ù‡ÙˆÙ† Ø§Ù„Ù…ÙˆØª Ø£Ù‡ÙˆÙ†\t\",\n",
        "    \"answer\": \"Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… Ø¹Ù„Ø§Ø¬ Ø§Ù„Ø§Ø¯Ù…Ø§Ù† Ù…Ø­ØªØ§Ø¬ Ø·Ø¨ÙŠØ¨ ÙˆØ§Ø¯ÙˆÙŠØ© ÙˆØ§Ù„Ø§Ù‡Ù… ÙŠÙƒÙˆÙ† Ø¹Ù†Ø¯Ùƒ Ø§Ø±Ø§Ø¯Ù‡ Ù‚ÙˆÙŠÙ‡ Ù„Ù„ØªØ®Ù„Øµ Ù…Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø³Ù…ÙˆÙ… ÙˆØ¹Ù„Ù‰ Ù‚Ø¯Ø± Ø¹Ø²ÙŠÙ…ØªÙƒ Ù‡ÙŠÙƒÙˆÙ† Ø§Ù„ØªÙˆÙÙŠÙ‚ Ù„Ù†Ø¬Ø§Ø­ Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø³Ø§Ø¹Ø¯ Ù†ÙØ³Ùƒ\t\"\n",
        "}\n",
        "\n",
        "example_3 = {\n",
        "    \"question\": \"Ø§Ø¹Ø§Ù†ÙŠ Ù…Ù† Ø§Ù„Ø®ÙˆÙ ÙˆØªÙ†Ø¨Ø¤Ø§Øª Ù…Ù† ÙƒÙ„Ø§Ù… Ø§Ù‡Ù„ÙŠ ÙˆØ¨ØµÙ…ØªÙ‡Ù… Ø§Ù†Ù‡Ù… ÙŠØ¨ÙˆÙ† ÙŠÙ‚ØªÙ„ÙˆÙ†ÙŠ Ø¨Ø¯ÙˆÙ† Ø³Ø¨Ø¨ ÙˆØ§Ù„ÙŠ Ù…Ø­Ø±Ø¶Ù‡Ù… Ø§Ø¨ÙˆÙŠ\",\n",
        "    \"answer\": \"Ø§Ù„ØªÙ‚Ù„Ø¨Ø§Øª Ø§Ù„Ù…Ø²Ø§Ø¬ÙŠÙ‡ ÙÙŠ Ø³Ù†Ùƒ Ø§Ù„Ù…Ø¨ÙƒØ±Ù‡ ÙƒØ«ÙŠØ±Ù‡ ÙˆÙ…Ù† Ø§Ù„Ù…ÙÙŠØ¯ Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ø·Ø¨ÙŠØ¨ Ù„Ù„ØªØ´Ø®ÙŠØµ Ø§Ù„Ø¯Ù‚ÙŠÙ‚\t\"\n",
        "}\n",
        "\n",
        "example_4 = {\n",
        "    \"question\": \"Ø¹Ù…Ø±Ù‡Ø§ 15 Ù…Ù†Ø° Ù‚Ø±Ø§Ø¨Ù‡ Ø´Ù‡Ø±ÙŠÙ† Ø§Ùˆ Ø§ÙƒØ«Ø± Ø¨Ø¯Ø§Øª Ø­Ø§Ù„ØªÙ‡Ø§ ØªØªØºÙŠØ± Ù…Ù† Ø§Ù„Ù‡Ø¯ÙˆØ¡ Ø§Ù„Ù‰ Ø§Ù„Ø¹ØµØ¨ÙŠÙ‡ ÙˆØ¹Ø¯Ù… Ø§Ù„Ù†ÙˆÙ…\",\n",
        "    \"answer\": \"Ù‡Ù†Ø§Ùƒ Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ø§Ø³Ø¦Ù„Ù‡ Ù‡Ù„ ÙŠÙˆØ¬Ø¯ Ø®Ù„Ø§ÙØ§Øª ÙÙŠ Ø§Ù„Ù…Ù†Ø²Ù„ Ù‡Ù„ ØªØ¹Ø±Ø¶Øª Ù„Ø§ÙŠ Ø¶ØºÙˆØ· Ø§Ùˆ Ø§ÙŠ ØµØ¯Ù…Ù‡ ØŸ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªÙˆØ§ØµÙ„ Ù…Ø¨Ø§Ø´Ø±Ù‡ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬ Ù†ÙØ³ÙŠ Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ù‡\"\n",
        "}\n",
        "\n",
        "example_5 = {\n",
        "    \"question\": \"Ù‚Ù„Ù‚ ÙˆØªÙˆØªØ± ÙˆÙˆØ³ÙˆØ§Ø³ Ù‚Ù‡Ø±ÙŠ ÙˆØªØ§ØªØ§Ù‡ ÙÙŠ Ø§Ù„ÙƒÙ„Ø§Ù… ÙˆØ¹Ø¬Ø² Ø¯Ø§Ø®Ù„ÙŠ Ø§Ø¸Ù† Ø§Ù†ÙŠ Ù…ÙƒØ±ÙˆÙ‡ ÙˆØ§Ù†ÙŠ Ù„Ø§ Ø§Ù‚Ø¯Ø± Ø§ÙØ¹Ù„ Ø´ÙŠ\t\",\n",
        "    \"answer\": \"Ø§Ù†Ø´ØºÙ„ Ø¨Ø§Ù„Ø¯Ø±Ø§Ø³Ù‡ Ø§ÙˆØ§Ù„Ø¹Ù…Ù„ Ø§Ùˆ Ø§Ù„Ø§Ù†Ø´Ø·Ù‡ Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠÙ‡ ÙˆÙ…Ø¹ ØªÙ„Ù‚ÙŠ Ø§Ù„Ø¹Ù„Ø§Ø¬ Ø§Ù„Ù†ÙØ³ÙŠ ØªØªØ­Ø³Ù† Ø§Ù„Ø­Ø§Ù„Ù‡ Ù…Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØµØ¨Ø±\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpZ6a4DpBD6z"
      },
      "outputs": [],
      "source": [
        "def build_fewshot_5_prompt(new_question):\n",
        "    prompt = f\"\"\"\n",
        "Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø¨Ø§Ø­ØªØ±Ø§ÙÙŠØ© ÙˆØ¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:\n",
        "\n",
        "Ø³: {example_1['question']}\n",
        "Ø¬: {example_1['answer']}\n",
        "\n",
        "Ø³: {example_2['question']}\n",
        "Ø¬: {example_2['answer']}\n",
        "\n",
        "Ø³: {example_3['question']}\n",
        "Ø¬: {example_3['answer']}\n",
        "\n",
        "Ø³: {example_4['question']}\n",
        "Ø¬: {example_4['answer']}\n",
        "\n",
        "Ø³: {example_5['question']}\n",
        "Ø¬: {example_5['answer']}\n",
        "\n",
        "Ø³: {new_question}\n",
        "Ø¬:\"\"\"\n",
        "    return prompt.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ATufkq9BF6D"
      },
      "outputs": [],
      "source": [
        "def generate_llama_fewshot_5_answer(question):\n",
        "    prompt = build_fewshot_5_prompt(question)\n",
        "    response = chat.invoke([\n",
        "        HumanMessage(content=prompt)\n",
        "    ])\n",
        "    return response.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOgUITyiBHfZ"
      },
      "outputs": [],
      "source": [
        "sample_df = df[['question_clean_2', 'answer_clean_2']].sample(n=10, random_state=42).copy()\n",
        "sample_df['llama_fewshot_5_answer'] = sample_df['question_clean_2'].apply(generate_llama_fewshot_5_answer)\n",
        "sample_df.to_csv(\"llama_fewshot_5_answer.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWNTH_dVBKye"
      },
      "outputs": [],
      "source": [
        "from bert_score import score\n",
        "\n",
        "P, R, F1 = score(\n",
        "    sample_df['llama_fewshot_5_answer'].tolist(),\n",
        "    sample_df['answer_clean_2'].tolist(),\n",
        "    lang=\"ar\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "sample_df['bertscore_fewshot_5'] = F1.tolist()\n",
        "sample_df.to_csv(\"llama_fewshot_5_answer.csv\", index=False)\n",
        "print(f\"âœ… 5 - Few-Shot BERTScore F1: {F1.mean().item():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSjr2bH3Bp_4"
      },
      "outputs": [],
      "source": [
        "def build_fewshot_cot_prompt(new_question):\n",
        "    prompt = f\"\"\"\n",
        "\n",
        "Question: I can't sleep and feel stressed all day. Do I have a mental illness?\n",
        "\n",
        "Let's analyze the condition gradually:\n",
        "1. Look at the symptoms: insomnia (inability to sleep) and constant stress.\n",
        "2. Associated with: disorders such as generalized anxiety or depression.\n",
        "3. To properly diagnose the condition: You must know a few things.\n",
        "4. To answer: Provide a professional answer that takes into account the above factors using one or more of the strategies:\n",
        "- providing information\n",
        "- Direct answers\n",
        "- Emotional support\n",
        "\n",
        "Answer: It is best to consult a psychologist to evaluate the condition. Lifestyle changes or behavioral therapy may help alleviate the symptoms.\n",
        "\n",
        "Make sure your answer is fully formatted in Arabic and specifically designed to diagnose mental health conditions\n",
        "\n",
        "Question: {new_question}\n",
        "Answer:\"\"\"\n",
        "    return prompt.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3dsvvIrB16B"
      },
      "outputs": [],
      "source": [
        "def generate_llama_fewshot_cot_answer(question):\n",
        "    prompt = build_fewshot_cot_prompt(question)\n",
        "    response = chat.invoke([\n",
        "        HumanMessage(content=prompt)\n",
        "    ])\n",
        "    return response.content.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPJPHyR3CBN1"
      },
      "outputs": [],
      "source": [
        "sample_df = df[['question_clean_2', 'answer_clean_2']].sample(n=10, random_state=42).copy()\n",
        "sample_df['llama_fewshot_cot_answer'] = sample_df['question_clean_2'].apply(generate_llama_fewshot_cot_answer)\n",
        "sample_df.to_csv(\"llama_fewshot_cot_answer.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATKtBQIBCJoU"
      },
      "outputs": [],
      "source": [
        "from bert_score import score\n",
        "\n",
        "P, R, F1 = score(\n",
        "    sample_df['llama_fewshot_cot_answer'].tolist(),\n",
        "    sample_df['answer_clean_2'].tolist(),\n",
        "    lang=\"ar\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "sample_df['bertscore_llama_fewshot_cot_answer'] = F1.tolist()\n",
        "sample_df.to_csv(\"llama_fewshot_cot_answer.csv\", index=False)\n",
        "print(f\"âœ… CoT BERTScore F1: {F1.mean().item():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6smlpaemKJLf"
      },
      "outputs": [],
      "source": [
        "!pip install -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8npfcN8YJ_79"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()  # Ø£Ø¯Ø®Ù„ ØªÙˆÙƒÙ† HF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "print(f\"Google Generative AI version: {genai.__version__}\")"
      ],
      "metadata": {
        "id": "9Wc2qF80rBek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import __version__ as hf_version\n",
        "print(f\"Hugging Face Hub version: {hf_version}\")"
      ],
      "metadata": {
        "id": "3tiRrF-qqDIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(f\"Python version: {sys.version}\")"
      ],
      "metadata": {
        "id": "yfZXXiUwqbGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICZWTSSDLUu6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Allam\n",
        "allam_model = AutoModelForCausalLM.from_pretrained(\"ALLaM-AI/ALLaM-7B-Instruct-preview\", torch_dtype=\"auto\", device_map=\"auto\")\n",
        "allam_tokenizer = AutoTokenizer.from_pretrained(\"ALLaM-AI/ALLaM-7B-Instruct-preview\")\n",
        "\n",
        "# Qwen\n",
        "qwen_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\")\n",
        "qwen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0Sk1UiXKPYR"
      },
      "outputs": [],
      "source": [
        "# FINAL aLLAm\n",
        "def gen_hf(model, tokenizer, prompt):\n",
        "    # 1. Tokenize the prompt and move to model's device\n",
        "    toks = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # 2. Generate the model output\n",
        "    out = model.generate(**toks, max_new_tokens=1024)\n",
        "\n",
        "    # 3. Decode only the generated part (exclude input prompt)\n",
        "    generated_tokens = out[0][toks[\"input_ids\"].shape[1]:]\n",
        "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4foyFyWK4x3"
      },
      "outputs": [],
      "source": [
        "# FINAL Qwen\n",
        "def gen_qwen(model, tokenizer, prompt):\n",
        "    # 1. Tokenize the prompt and move to model's device\n",
        "    toks = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # 2. Generate output\n",
        "    out = model.generate(**toks, max_new_tokens=1024)\n",
        "\n",
        "    # 3. Decode only the generated part\n",
        "    generated_tokens = out[0][toks[\"input_ids\"].shape[1]:]\n",
        "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLYn5WmUKZQ7"
      },
      "outputs": [],
      "source": [
        "# FINAL Gemini\n",
        "import google.generativeai as genai\n",
        "genai.configure(api_key=\"..\")\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "def gen_gemini(model, prompt):\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaccnL0cKobq"
      },
      "outputs": [],
      "source": [
        "#FINAL GPT\n",
        "\n",
        "import openai\n",
        "\n",
        "openai.api_key = \"sk-proj....\"\n",
        "openai_model = \"gpt-4o\"\n",
        "\n",
        "def gen_openai(prompt, model_name):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QobDXnp5Kbqz"
      },
      "outputs": [],
      "source": [
        "# FINAL\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from bert_score import score\n",
        "\n",
        "# Load 15 samples\n",
        "#df = pd.read_csv(\"Train_Dev.tsv\", sep=\"\\t\")\n",
        "df_sample = df[['question_clean_2', 'answer_clean_2']].dropna().sample(n=20, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Prepare all prompts\n",
        "df_sample[\"prompt\"] = df_sample[\"question_clean_2\"].apply(build_fewshot_prompt)\n",
        "\n",
        "# Initialize result containers\n",
        "results = {\n",
        "    \"allam\": [],\n",
        "    \"qwen\": [],\n",
        "    \"gpt\": [],\n",
        "    \"gemini\": []\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xKqZbY3MKWVN"
      },
      "outputs": [],
      "source": [
        "#FINAL\n",
        "\n",
        "# Allam\n",
        "for prompt in tqdm(df_sample[\"prompt\"], desc=\"Allam\"):\n",
        "    ans = gen_hf(allam_model, allam_tokenizer, prompt)\n",
        "    results[\"allam\"].append(ans)\n",
        "\n",
        "# Qwen\n",
        "for prompt in tqdm(df_sample[\"prompt\"], desc=\"Qwen\"):\n",
        "    ans = gen_qwen(qwen_model, qwen_tokenizer, prompt)\n",
        "    results[\"qwen\"].append(ans)\n",
        "\n",
        "# OpenAI GPT\n",
        "for prompt in tqdm(df_sample[\"prompt\"], desc=\"GPT\"):\n",
        "    ans = gen_openai(prompt, openai_model)\n",
        "    results[\"gpt\"].append(ans)\n",
        "\n",
        "# Gemini\n",
        "for prompt in tqdm(df_sample[\"prompt\"], desc=\"Gemini\"):\n",
        "    ans = gen_gemini(gemini_model, prompt)\n",
        "    results[\"gemini\"].append(ans)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ruc7cPq9XDz_"
      },
      "outputs": [],
      "source": [
        "bert_results = {}\n",
        "\n",
        "for model_name in results.keys():\n",
        "    print(f\"Scoring {model_name}...\")\n",
        "    P, R, F1 = score(\n",
        "        results[model_name],\n",
        "        df_sample[\"answer_clean_2\"].tolist(),\n",
        "        lang=\"ar\",\n",
        "        verbose=True\n",
        "    )\n",
        "    bert_results[model_name] = {\n",
        "        \"precision\": P.mean().item(),\n",
        "        \"recall\": R.mean().item(),\n",
        "        \"f1\": F1.mean().item()\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2j9oNRMY-Em"
      },
      "outputs": [],
      "source": [
        "print(\"\\nğŸ“Š Final Average BERTScore F1 per Model:\")\n",
        "for model_name, scores in bert_results.items():\n",
        "    print(f\"{model_name}: F1 = {scores['f1']:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
